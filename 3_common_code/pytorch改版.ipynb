{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Point, Polygon\n",
    "import random\n",
    "from matplotlib.path import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  创建数据集\n",
    "Data_count = 0\n",
    "Data_set = []\n",
    "Labels = []\n",
    "# 数据集大小 500个\n",
    "Num_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数定义\n",
    "def generate_random_matrix(matrix_shape):\n",
    "    return np.random.uniform(-0.1, 0.1, size=matrix_shape)\n",
    "\n",
    "def generate_data():\n",
    "    #############生成数据的主函数####################\n",
    "    # 第一步：生成间距为0.5距离的点，加上正负0.15的偏移，点尺寸设置为0.5\n",
    "    spacing = 0.8\n",
    "    offset = 0.3\n",
    "    point_size = 0.5\n",
    "\n",
    "    x_coords = np.arange(0, 10, spacing)\n",
    "    y_coords = np.arange(0, 10, spacing)\n",
    "\n",
    "    x_grid, y_grid = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    x_grid += np.random.uniform(-offset, offset, size=x_grid.shape)\n",
    "    y_grid += np.random.uniform(-offset, offset, size=y_grid.shape)\n",
    "\n",
    "    points = np.column_stack((x_grid.flatten(), y_grid.flatten()))\n",
    "\n",
    "    # 第二步：随机生成几个点构成闭包区域，并删除在闭包区域内部的第一步生成的点\n",
    "    #################################\n",
    "    num_hull_points = np.random.randint(4, 10)\n",
    "    \n",
    "    hull_indices = np.random.choice(len(points), size=num_hull_points, replace=False)\n",
    "    hull_points = points[hull_indices]\n",
    "\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(hull_points)\n",
    "    # 获取凸包的边界点\n",
    "    hull_vertices = hull_points[hull.vertices]\n",
    "    # 使用Shapely库创建凸包多边形对象\n",
    "    polygon = Polygon(hull_vertices)\n",
    "    \n",
    "    # 删除在闭包区域内部的点\n",
    "    outside_points = []\n",
    "    for point in points:\n",
    "        if not polygon.contains(Point(point)):\n",
    "            outside_points.append(point)\n",
    "    \n",
    "    outside_points = np.array(outside_points)\n",
    "    ####################################\n",
    "    convex_hull_path = Path(hull_vertices)\n",
    "    def generate_points_in_convex_hull(convex_hull, min_spacing, max_spacing):\n",
    "        min_x = np.min(convex_hull[:, 0])\n",
    "        max_x = np.max(convex_hull[:, 0])\n",
    "        min_y = np.min(convex_hull[:, 1])\n",
    "        max_y = np.max(convex_hull[:, 1])\n",
    "\n",
    "        num_points_x = int((max_x - min_x) / min_spacing)\n",
    "        num_points_y = int((max_y - min_y) / min_spacing)\n",
    "\n",
    "        points = np.meshgrid(np.linspace(min_x, max_x, num_points_x),\n",
    "                             np.linspace(min_y, max_y, num_points_y))\n",
    "        points = np.vstack((points[0].flatten(), points[1].flatten())).T\n",
    "\n",
    "        mask = convex_hull_path.contains_points(points)\n",
    "        points = points[mask]\n",
    "        return points\n",
    "    \n",
    "    min_spacing_inside = 0.8\n",
    "    max_spacing_inside = 0.82\n",
    "    inside_points = generate_points_in_convex_hull(hull_vertices, min_spacing_inside, max_spacing_inside)\n",
    "    # 再加上随机扰动\n",
    "    inside_points = generate_random_matrix(inside_points.shape) + inside_points  \n",
    "    # 生成返回值列表\n",
    "    temp_data = []\n",
    "    temp_label = []\n",
    "    # 最后再对数据进行判断，如果错误返回 0\n",
    "    if (len(outside_points)+len(inside_points))>160 or len(inside_points)<5:\n",
    "            return generate_data()\n",
    "    # 如果数据没有错误就好好处理一下\n",
    "    else:\n",
    "        # 总的数据数组  \n",
    "        list_temp_allData = []\n",
    "        for i in outside_points:\n",
    "            # 晶体\n",
    "            list_temp_allData.append([i[0],i[1],1])\n",
    "        for j in inside_points:\n",
    "            # 玻璃\n",
    "            list_temp_allData.append([j[0],j[1],2])\n",
    "        # 再补上 补全数据\n",
    "        for i in range(160-len(outside_points)-len(inside_points)):\n",
    "            list_temp_allData.append([0,0,0])\n",
    "\n",
    "        # 打乱一下数据\n",
    "        random.shuffle(list_temp_allData)\n",
    "        # 生成标签\n",
    "        for i in list_temp_allData:\n",
    "            temp_data.append([i[0],i[1]])\n",
    "            temp_label.append(i[2])\n",
    "        return temp_data,temp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "for item in range(Num_samples):\n",
    "    temp_data,temp_label = generate_data()\n",
    "    Data_set.append(temp_data)\n",
    "    Labels.append(temp_label)\n",
    "print(\"Data is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 160)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(Data_set).reshape(5000,-1)[:4500]\n",
    "y_train = np.array(Labels)[:4500]\n",
    "x_valid = np.array(Data_set).reshape(5000,-1)[4500:]\n",
    "y_valid = np.array(Labels)[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.8977,  7.9685,  5.9239,  ...,  5.3522,  6.6813,  3.2196],\n",
      "        [ 2.6523,  4.8530,  0.5672,  ...,  2.2472,  6.2472,  6.4433],\n",
      "        [ 9.6019,  2.9026,  9.7297,  ...,  2.7398,  1.3080,  2.9570],\n",
      "        ...,\n",
      "        [ 7.8173,  3.3473,  4.3956,  ...,  0.2570, -0.0945,  7.0483],\n",
      "        [ 4.7262,  4.5044,  6.4294,  ...,  3.4753,  5.5523,  0.9869],\n",
      "        [ 0.2487,  8.7014,  5.3802,  ...,  1.8225,  6.1407,  1.5217]],\n",
      "       dtype=torch.float64) tensor([[1, 2, 2,  ..., 2, 1, 2],\n",
      "        [1, 1, 2,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 2, 1],\n",
      "        ...,\n",
      "        [1, 2, 1,  ..., 1, 1, 1],\n",
      "        [2, 1, 2,  ..., 1, 2, 1],\n",
      "        [1, 1, 0,  ..., 1, 1, 1]], dtype=torch.int32)\n",
      "torch.Size([4500, 320])\n",
      "tensor(0, dtype=torch.int32) tensor(2, dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_28772\\3508818229.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import torch\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "n, c = x_train.shape\n",
    "\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.functional 很多层和函数在这里都会见到\n",
    "\n",
    "torch.nn.functional中有很多功能，后续会常用的。那什么时候使用nn.Module，什么时候使用nn.functional呢？一般情况下，如果模型有可学习的参数，最好用nn.Module，其他情况nn.functional相对更简单一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb.mm(weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x320 and 512x160)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      6\u001b[0m bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m160\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_func(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m, yb))\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(xb)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel\u001b[39m(xb):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m bias\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x320 and 512x160)"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "yb = y_train[0:bs]\n",
    "weights = torch.randn([512, 160], dtype = torch.float,  requires_grad = True) \n",
    "bs = 64\n",
    "bias = torch.zeros(160, requires_grad=True)\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个model来更简化代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 必须继承nn.Module且在其构造函数中需调用nn.Module的构造函数\n",
    "- 无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播\n",
    "- Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(320, 1024)\n",
    "        self.hidden2 = nn.Linear(1024, 512)\n",
    "        self.out  = nn.Linear(512, 160)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_NN(\n",
      "  (hidden1): Linear(in_features=320, out_features=1024, bias=True)\n",
      "  (hidden2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=160, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Mnist_NN()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以打印我们定义好名字里的权重和偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1.weight Parameter containing:\n",
      "tensor([[-0.0256,  0.0160,  0.0312,  ...,  0.0127, -0.0094, -0.0095],\n",
      "        [ 0.0011, -0.0442,  0.0483,  ..., -0.0059, -0.0503,  0.0430],\n",
      "        [ 0.0101,  0.0287,  0.0552,  ..., -0.0435,  0.0329, -0.0348],\n",
      "        ...,\n",
      "        [ 0.0341,  0.0217, -0.0243,  ..., -0.0414, -0.0260, -0.0354],\n",
      "        [-0.0110, -0.0229, -0.0050,  ...,  0.0216,  0.0532,  0.0458],\n",
      "        [ 0.0058,  0.0049,  0.0492,  ..., -0.0053, -0.0539, -0.0046]],\n",
      "       requires_grad=True) torch.Size([1024, 320])\n",
      "hidden1.bias Parameter containing:\n",
      "tensor([-0.0139, -0.0339,  0.0252,  ...,  0.0252,  0.0419,  0.0304],\n",
      "       requires_grad=True) torch.Size([1024])\n",
      "hidden2.weight Parameter containing:\n",
      "tensor([[ 0.0027,  0.0300, -0.0223,  ..., -0.0085,  0.0145, -0.0080],\n",
      "        [ 0.0074, -0.0114,  0.0282,  ..., -0.0309, -0.0212,  0.0070],\n",
      "        [-0.0147, -0.0109, -0.0303,  ..., -0.0138, -0.0196, -0.0228],\n",
      "        ...,\n",
      "        [-0.0150, -0.0258, -0.0072,  ..., -0.0012, -0.0088,  0.0052],\n",
      "        [-0.0151, -0.0022, -0.0206,  ...,  0.0044, -0.0146, -0.0112],\n",
      "        [ 0.0284,  0.0103, -0.0292,  ...,  0.0126,  0.0010, -0.0233]],\n",
      "       requires_grad=True) torch.Size([512, 1024])\n",
      "hidden2.bias Parameter containing:\n",
      "tensor([-1.1843e-02,  1.4426e-02,  1.3007e-03, -2.7785e-02, -1.1241e-02,\n",
      "         3.4723e-03, -1.5246e-02, -3.0125e-02, -9.3776e-03,  2.7891e-02,\n",
      "        -2.7806e-02,  1.4715e-03, -1.0145e-02, -3.3147e-03,  3.3328e-04,\n",
      "        -6.8534e-03, -1.5052e-02,  7.0401e-03, -7.6828e-03,  2.0805e-02,\n",
      "         2.3302e-02,  6.5938e-03, -2.6386e-02,  2.0748e-02,  2.5302e-02,\n",
      "        -1.2567e-02, -1.5463e-02,  1.4451e-02,  2.7820e-02,  7.9235e-03,\n",
      "         1.8247e-02,  1.7490e-03, -3.0463e-02,  1.5192e-02, -2.3789e-02,\n",
      "        -2.3512e-02, -2.6123e-02,  5.3180e-03, -2.4541e-02,  1.2017e-02,\n",
      "        -2.4921e-02, -2.3078e-02, -1.6392e-03,  3.8767e-03,  2.2774e-02,\n",
      "         2.4687e-02,  5.5816e-03, -1.3914e-02,  2.0561e-02,  9.2680e-03,\n",
      "         1.4912e-02,  8.1591e-03,  2.5732e-04,  4.9474e-03,  1.8445e-02,\n",
      "         1.4150e-02,  2.6491e-02, -2.3935e-02, -6.5268e-03, -2.8668e-02,\n",
      "         2.7244e-02,  2.9690e-02,  2.5578e-02,  1.2025e-02,  1.4598e-03,\n",
      "        -1.4165e-02, -6.3597e-03,  1.8506e-02,  8.6446e-03, -4.4260e-03,\n",
      "        -1.6628e-02,  2.5036e-03, -2.3874e-02, -7.6003e-03,  1.6944e-02,\n",
      "         1.4780e-02, -7.2633e-03, -5.6099e-03, -3.8314e-03,  2.2959e-02,\n",
      "        -3.5359e-03,  2.4341e-02,  2.2971e-03,  2.5316e-02,  1.2548e-02,\n",
      "        -4.6863e-03, -3.0072e-02, -2.7797e-02, -2.9406e-02,  3.0469e-02,\n",
      "        -9.7934e-03,  2.3335e-02, -2.4758e-02, -2.0178e-02, -2.7322e-02,\n",
      "        -1.4789e-03, -1.3221e-02, -1.5915e-02,  2.3163e-03, -1.3199e-02,\n",
      "         3.6816e-03,  1.7878e-02, -1.3368e-02, -2.4148e-02,  3.0276e-02,\n",
      "         1.5976e-02, -2.1212e-02,  2.9246e-02,  2.6358e-02, -2.1146e-02,\n",
      "         1.9109e-02, -2.8909e-03, -3.0299e-02,  5.6286e-03, -1.4704e-02,\n",
      "        -2.1288e-02,  2.4605e-02, -2.4030e-02, -1.5397e-02, -2.8477e-02,\n",
      "        -2.0119e-03,  1.2095e-02,  2.3365e-02, -2.4031e-02,  3.1066e-02,\n",
      "        -9.4854e-04,  1.4819e-02, -1.3003e-03,  2.0240e-02, -1.1903e-03,\n",
      "         2.9576e-02, -1.7068e-03,  1.2800e-02, -2.8067e-02, -1.3636e-02,\n",
      "        -8.8085e-03, -6.0219e-03, -2.0935e-02, -1.6274e-02,  4.4873e-03,\n",
      "         5.1719e-03, -2.0845e-02, -2.1328e-02, -2.7512e-02,  2.2679e-02,\n",
      "        -2.0778e-03,  1.5884e-02, -2.9608e-02, -2.3430e-02, -1.6798e-02,\n",
      "         5.5214e-04,  1.2185e-02,  1.3659e-02, -2.4524e-02,  3.9499e-03,\n",
      "         2.9668e-03,  1.7869e-02, -2.5402e-02,  2.9158e-02,  1.8570e-02,\n",
      "        -2.4495e-02,  1.7206e-02, -4.6272e-03, -7.4391e-04,  1.0669e-02,\n",
      "         3.0096e-02,  2.9351e-02,  9.6173e-03, -1.0153e-02,  3.0647e-02,\n",
      "         2.3125e-02,  2.8063e-02, -2.5124e-03, -9.8223e-03,  1.2814e-02,\n",
      "         1.4637e-02, -8.9575e-03, -1.1499e-02, -1.0184e-02,  1.3404e-02,\n",
      "         9.9342e-03, -1.2433e-02,  8.5311e-03, -2.3660e-02,  3.2914e-03,\n",
      "         8.9504e-03, -2.1598e-02, -1.6432e-02,  5.2399e-03, -5.8554e-03,\n",
      "        -9.8991e-03,  2.2252e-02,  2.5418e-02, -1.2798e-02,  1.6636e-02,\n",
      "        -1.0822e-02, -1.6783e-02,  1.7565e-02, -1.0588e-02,  1.5679e-02,\n",
      "        -2.6662e-02,  2.8714e-02,  1.7430e-02, -2.6285e-02, -4.4715e-03,\n",
      "        -2.0407e-02, -2.3019e-02,  1.4003e-02,  2.2051e-02,  6.2517e-03,\n",
      "        -2.2278e-02, -8.0801e-03, -6.9457e-03,  1.7408e-02, -1.8974e-02,\n",
      "         1.5528e-02, -2.1823e-02,  1.5000e-02, -1.1487e-02, -2.9480e-03,\n",
      "         2.4618e-02,  1.8027e-03, -2.5216e-02, -2.4607e-02,  8.0062e-03,\n",
      "        -1.2220e-02, -2.8678e-02, -2.8385e-02, -1.5629e-02, -2.8201e-02,\n",
      "         2.0585e-02,  9.0296e-03,  1.6145e-04,  1.5613e-02,  1.9401e-03,\n",
      "         6.8900e-04,  2.2840e-03, -1.4803e-03,  1.9766e-02,  1.2842e-02,\n",
      "        -2.7461e-02,  1.3776e-02, -1.5811e-02, -2.7659e-02,  6.7302e-03,\n",
      "        -2.7217e-02, -1.0966e-02, -7.7421e-03,  1.9764e-02,  1.6866e-02,\n",
      "         3.0590e-02,  1.9838e-03,  1.7721e-02,  7.4666e-03,  6.5382e-03,\n",
      "         2.0906e-03, -1.7795e-02,  1.4342e-03, -6.8986e-03, -5.9033e-03,\n",
      "         2.5214e-02, -2.5346e-02,  2.9589e-02,  2.0750e-02, -1.8531e-02,\n",
      "         3.0068e-02,  2.7389e-03,  2.9557e-02,  1.4599e-02,  1.4014e-02,\n",
      "         2.2646e-02, -2.7420e-02, -1.8434e-02, -2.6678e-03,  5.9607e-03,\n",
      "        -2.2120e-02,  8.0453e-03,  2.5919e-02,  1.2534e-03,  2.9044e-02,\n",
      "         1.6627e-02, -7.7422e-03, -2.2720e-02,  6.3286e-03,  2.9593e-02,\n",
      "         2.7969e-02, -5.4784e-03,  4.9017e-03,  8.6364e-03, -1.7940e-02,\n",
      "         2.7774e-02, -1.8423e-02, -2.8358e-02,  2.2646e-02,  2.7153e-02,\n",
      "         3.1020e-02,  2.3563e-02,  2.9218e-02,  1.4051e-02, -7.6549e-03,\n",
      "        -1.1480e-02,  3.0916e-02,  2.3951e-02, -1.6159e-02, -1.3615e-03,\n",
      "         2.9556e-02, -2.2735e-02,  1.6007e-03, -2.5918e-02,  2.9403e-02,\n",
      "        -1.8655e-02,  1.1199e-03,  1.5351e-02, -2.1666e-02,  1.3782e-02,\n",
      "         7.1830e-03, -2.2986e-02, -1.0607e-02,  1.8364e-02, -1.0294e-02,\n",
      "        -2.9975e-02,  1.3427e-02, -3.0814e-02, -2.4499e-02, -2.0369e-02,\n",
      "         6.6683e-03, -2.0166e-02, -8.6402e-03,  1.9163e-02,  2.0517e-02,\n",
      "         2.5569e-02,  4.6719e-04, -9.5103e-03, -1.2651e-02, -9.8196e-03,\n",
      "         1.8520e-02, -7.8050e-03, -2.4049e-02, -5.1990e-03,  2.3320e-02,\n",
      "         1.5758e-02, -2.7329e-03, -1.3944e-02,  7.3120e-03, -2.7105e-02,\n",
      "         1.8564e-02, -5.4945e-03,  1.2192e-02, -4.8189e-03,  3.0252e-03,\n",
      "        -7.0798e-03,  2.6473e-03,  1.4884e-02,  2.6548e-02,  1.9110e-02,\n",
      "        -3.1170e-02,  3.0457e-02,  3.9890e-03, -9.3139e-03,  7.5286e-03,\n",
      "         2.7592e-02,  2.3657e-02,  2.5359e-02, -6.5188e-03, -2.0673e-02,\n",
      "        -3.0796e-03, -2.0912e-02,  1.7234e-02,  2.7790e-02, -3.0947e-02,\n",
      "         2.7046e-02, -2.7767e-02, -2.3591e-02,  1.6270e-02,  1.5379e-03,\n",
      "        -2.3424e-02, -6.1465e-03, -9.9593e-03, -2.6397e-02,  1.2955e-02,\n",
      "        -2.2019e-02,  2.3130e-02, -2.3372e-02, -2.1794e-03,  2.9187e-02,\n",
      "        -5.1982e-03,  2.5166e-02,  3.1171e-02, -5.9080e-03,  1.9086e-03,\n",
      "         2.1567e-02, -2.2778e-02,  1.1616e-02, -2.7040e-02,  2.5415e-02,\n",
      "         2.4928e-02,  1.0940e-02,  4.5668e-04,  2.8803e-02,  9.0360e-03,\n",
      "        -8.2027e-03,  6.9118e-03,  1.6457e-02,  9.0689e-03,  7.1834e-03,\n",
      "        -3.2887e-03, -2.6149e-02,  5.6215e-03,  4.1456e-03,  2.2199e-03,\n",
      "         1.8839e-03, -5.4039e-03,  1.1116e-02, -2.9932e-02,  1.4063e-02,\n",
      "        -2.8523e-02, -1.2467e-02, -1.4416e-02,  2.7178e-02,  2.8785e-02,\n",
      "         1.9128e-02,  1.4558e-03,  2.7846e-02, -2.9814e-02, -2.0632e-02,\n",
      "        -1.7932e-02, -2.3763e-02,  6.3914e-03,  8.6441e-03,  2.1629e-02,\n",
      "         4.7068e-03,  2.9260e-02, -2.8559e-02,  9.7949e-05,  5.5839e-03,\n",
      "         1.6485e-02,  2.1105e-02, -1.7748e-02,  7.5655e-03,  1.2045e-02,\n",
      "        -2.6701e-02,  1.6413e-02,  1.1674e-02,  1.5937e-02,  1.8358e-02,\n",
      "        -1.5351e-02,  8.0321e-03, -3.5414e-03, -8.9006e-03, -1.1567e-02,\n",
      "        -1.4269e-02, -1.4857e-02,  2.8694e-02,  2.7582e-02, -9.9687e-03,\n",
      "         1.1700e-02,  2.1959e-02, -8.1004e-03, -3.5972e-03, -1.4743e-03,\n",
      "         2.0368e-02, -1.8834e-02, -1.7516e-02,  2.6894e-02,  7.5319e-03,\n",
      "        -2.1217e-02, -3.1108e-02,  1.1198e-02, -2.9711e-02,  2.6921e-02,\n",
      "        -1.9968e-02, -4.4138e-03,  2.0903e-02,  1.5262e-03, -1.9307e-02,\n",
      "        -2.2968e-03, -2.5005e-02, -1.3200e-03, -1.6095e-02, -9.7141e-03,\n",
      "         6.0336e-03, -1.4900e-02, -6.5140e-03,  2.1265e-02, -2.3745e-02,\n",
      "        -1.7260e-02, -2.9140e-02,  1.3519e-03,  2.1564e-02, -2.8456e-02,\n",
      "        -1.9313e-02,  1.2429e-02,  2.1770e-02, -1.1809e-02,  1.6651e-02,\n",
      "        -1.8317e-02,  1.7763e-02,  2.4686e-02,  2.8844e-02,  2.8626e-02,\n",
      "        -2.6425e-04,  1.3519e-02,  3.1202e-02, -1.2553e-02,  2.0790e-02,\n",
      "        -9.9676e-03, -7.4064e-04, -5.4290e-04,  2.1599e-03, -4.5350e-03,\n",
      "         1.7908e-02, -1.9625e-03], requires_grad=True) torch.Size([512])\n",
      "out.weight Parameter containing:\n",
      "tensor([[-0.0289, -0.0337, -0.0021,  ...,  0.0199,  0.0282, -0.0387],\n",
      "        [ 0.0133, -0.0038, -0.0115,  ...,  0.0314,  0.0258,  0.0096],\n",
      "        [-0.0129,  0.0076, -0.0432,  ...,  0.0102, -0.0012,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0388,  0.0046,  ...,  0.0147,  0.0295,  0.0376],\n",
      "        [-0.0263, -0.0121, -0.0436,  ...,  0.0057, -0.0265, -0.0432],\n",
      "        [ 0.0064,  0.0140,  0.0112,  ...,  0.0050,  0.0300, -0.0095]],\n",
      "       requires_grad=True) torch.Size([160, 512])\n",
      "out.bias Parameter containing:\n",
      "tensor([ 1.1707e-02,  3.2225e-02, -4.1576e-02,  1.7380e-02, -1.0763e-02,\n",
      "        -2.8918e-02,  3.0858e-02, -1.9341e-02, -1.4373e-02, -1.5124e-02,\n",
      "         3.3383e-02,  4.2781e-03,  2.6574e-02,  3.7666e-02, -2.2709e-02,\n",
      "        -1.2707e-02, -2.6295e-02, -2.4816e-02,  3.6839e-02,  2.6506e-02,\n",
      "         1.9562e-02,  2.5763e-02,  3.1362e-02, -1.7731e-02, -4.3744e-02,\n",
      "         4.1505e-02,  3.6671e-03,  1.9948e-02, -4.2464e-02, -3.4436e-02,\n",
      "         3.3133e-03,  2.7657e-02,  3.1246e-02,  2.6086e-02,  1.0329e-02,\n",
      "         3.2715e-03, -4.4990e-03,  9.2074e-03,  3.3056e-02,  2.2509e-02,\n",
      "        -1.8964e-02,  2.5362e-02,  3.5085e-02,  1.8741e-02,  1.3255e-02,\n",
      "         2.9050e-02,  1.3680e-02,  1.7666e-02, -2.3981e-02, -3.8292e-02,\n",
      "        -2.6483e-02, -5.0003e-03,  3.3877e-02, -4.1661e-02,  1.5594e-02,\n",
      "        -2.6180e-03,  3.5604e-02, -1.2781e-02,  3.3885e-02, -3.0024e-02,\n",
      "        -4.1582e-02,  3.0925e-02, -2.3635e-02,  1.0483e-02,  3.7256e-02,\n",
      "         3.8046e-02, -5.1434e-03, -3.3458e-02,  2.3734e-02,  3.8117e-02,\n",
      "         2.4892e-04, -4.3281e-02, -3.2155e-02,  7.4088e-04,  3.6655e-02,\n",
      "        -4.0148e-02, -2.1611e-02,  7.8877e-03, -2.4241e-02, -2.0041e-02,\n",
      "         4.3194e-03, -3.3566e-02,  3.3743e-02,  1.4400e-02, -1.9565e-02,\n",
      "        -3.6192e-02,  3.2867e-02,  3.2570e-05,  5.4511e-03,  3.1538e-03,\n",
      "         9.4367e-04, -1.8917e-02,  2.2114e-02, -2.5504e-02, -9.8762e-03,\n",
      "         1.8849e-02,  2.5338e-02, -1.3927e-02, -3.7314e-02,  3.9735e-02,\n",
      "         2.6581e-02,  1.8694e-02,  2.4620e-02,  1.7976e-02,  2.5035e-02,\n",
      "         3.2381e-03, -5.4543e-04,  1.5516e-02, -3.4785e-02, -3.9138e-02,\n",
      "         1.9844e-02,  8.7446e-03,  5.3415e-03, -3.6608e-02, -1.1694e-02,\n",
      "        -1.1232e-03, -3.8081e-02, -3.8069e-02,  2.9276e-03,  4.0520e-02,\n",
      "        -2.8275e-02, -2.0325e-02, -1.1118e-02, -9.2275e-03,  3.4495e-02,\n",
      "         4.0957e-02,  4.1918e-03, -1.3403e-02, -2.3189e-02,  2.6427e-02,\n",
      "         3.2722e-02,  4.3238e-02,  1.9097e-02,  4.0524e-02, -3.0730e-02,\n",
      "        -2.4945e-02, -3.1615e-02,  3.0876e-02,  2.0374e-03,  3.4270e-02,\n",
      "         4.2724e-02, -3.0901e-02, -2.4479e-02, -9.0906e-04, -9.9648e-04,\n",
      "        -4.0988e-02,  1.9902e-02, -1.9779e-02,  3.2819e-02,  6.4719e-03,\n",
      "         4.6635e-04,  3.8026e-02, -3.7530e-02, -2.8206e-02, -3.4753e-02,\n",
      "        -3.8183e-02,  6.7483e-03,  3.8735e-02, -3.5389e-03, -1.0007e-02],\n",
      "       requires_grad=True) torch.Size([160])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter,parameter.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用TensorDataset和DataLoader来简化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一般在训练模型时加上model.train()，这样会正常使用Batch Normalization和 Dropout\n",
    "- 测试的时候一般选择model.eval()，这样就不会使用Batch Normalization和 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for step in range(steps):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print('当前step:'+str(step), '验证集损失：'+str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "    model = Mnist_NN()\n",
    "    return model, optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三行搞定！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dl, valid_dl \u001b[38;5;241m=\u001b[39m get_data(train_ds, valid_ds, bs)\n\u001b[0;32m      2\u001b[0m model, opt \u001b[38;5;241m=\u001b[39m get_model()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(steps, model, loss_func, opt, train_dl, valid_dl)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mloss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mloss_batch\u001b[1;34m(model, loss_func, xb, yb, opt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_batch\u001b[39m(model, loss_func, xb, yb, opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m, yb)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mD:\\python_install\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mMnist_NN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2(x))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n",
      "File \u001b[1;32mD:\\python_install\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\python_install\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(25, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
